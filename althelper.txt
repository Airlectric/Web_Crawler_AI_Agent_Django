from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
import logging
from typing import Set, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import tldextract
import backoff
import re
import os
import time
from sentence_transformers import SentenceTransformer, util
import numpy as np
import utils.state 
from utils.config import load_config
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from threading import Lock
from contextlib import contextmanager

logger = logging.getLogger(__name__)

config = load_config()

REQUEST_TIMEOUT = config['REQUEST_TIMEOUT']
MAX_WORKERS = config['MAX_WORKERS']
MAX_DEPTH = config['MAX_DEPTH']
MAX_URLS = config['MAX_URLS']
TIMEOUT_SECONDS = config['TIMEOUT_SECONDS']

SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
})

start_time = time.time()

# Load sentence-transformers model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Updated keywords targeting science lab solutions and innovative research
LAB_KEYWORDS = [
    "science lab", "research lab", "laboratory", "experiment", "scientific research",
    "innovation", "global challenges", "scientific solutions", "technology transfer",
    "patents", "research projects", "sustainability", "climate change", "energy",
    "health", "biotechnology", "nanotechnology", "environmental science",
    "renewable energy", "water purification", "disease prevention", "food security",
    "faculty", "department", "science", "chemistry"
]
ACADEMIC_KEYWORDS = ["faculty", "department", "science", "research", "lab", "laboratory", "institute", "faculties", "departments"]

# Pre-computed category embeddings
lab_embedding = model.encode(" ".join(LAB_KEYWORDS))
STARTUP_KEYWORDS = ["startup", "venture", "accelerator", "incubator", "entrepreneur", "funding"]
startup_embedding = model.encode(" ".join(STARTUP_KEYWORDS))

# Selenium driver management
SELENIUM_LOCK = Lock()
SELENIUM_DRIVER = None

@contextmanager
def get_selenium_driver():
    """Manage a single Selenium driver instance."""
    global SELENIUM_DRIVER
    with SELENIUM_LOCK:
        if SELENIUM_DRIVER is None:
            options = Options()
            options.add_argument("--headless=new")  # New headless mode
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--disable-dev-shm-usage")
            SELENIUM_DRIVER = webdriver.Chrome(options=options)
            logging.info("Initialized Selenium driver in headless mode")
        yield SELENIUM_DRIVER

def cleanup_selenium_driver():
    """Clean up the Selenium driver on program exit."""
    global SELENIUM_DRIVER
    with SELENIUM_LOCK:
        if SELENIUM_DRIVER is not None:
            SELENIUM_DRIVER.quit()
            SELENIUM_DRIVER = None
            logging.info("Cleaned up Selenium driver")

### Utility Functions
def load_university_domains(file_path: str = 'data/universities.txt') -> Set[str]:
    """Load and validate university domains."""
    logging.info(f"Loading university domains from {file_path}")
    domains = set()
    try:
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parsed = tldextract.extract(line)
                if parsed.registered_domain:
                    domains.add(parsed.registered_domain.lower())
                else:
                    logging.warning(f"Invalid domain format: {line}")
        logging.info(f"Loaded {len(domains)} validated university domains")
        return domains
    except FileNotFoundError:
        logging.error(f"University domains file not found: {file_path}")
        return set()

def is_university_domain(url: str, university_domains: Set[str]) -> bool:
    """Check if a URL belongs to a university domain, including subdomains."""
    extracted = tldextract.extract(url)
    domain = extracted.registered_domain.lower()
    return domain in university_domains

@backoff.on_exception(
    backoff.expo,
    (requests.exceptions.RequestException, requests.exceptions.Timeout),
    max_tries=3,
    jitter=backoff.full_jitter(60))
def fetch_url(url: str) -> Tuple[str, str]:
    """Fetch URL content with robust error handling, using Selenium for academic URLs."""
    # Try static fetch first
    try:
        response = SESSION.get(url, timeout=REQUEST_TIMEOUT)
        if response.status_code == 404:
            logging.warning(f"URL not found (404): {url}")
            return (url, None)
        response.raise_for_status()
        logging.debug(f"Static fetch successful for {url}")
        return (url, response.text)
    except Exception as e:
        # Only use Selenium for URLs likely to be academic
        path = urlparse(url).path.lower()
        if not any(kw in path for kw in ACADEMIC_KEYWORDS):
            logging.debug(f"Skipping Selenium for non-academic URL: {url}")
            return (url, None)
        logging.warning(f"Static fetch failed for {url}: {str(e)}, trying Selenium")
        try:
            with get_selenium_driver() as driver:
                driver.get(url)
                # Wait for dynamic content to load (e.g., navigation menus)
                WebDriverWait(driver, 5).until(
                    EC.presence_of_element_located((By.TAG_NAME, "a"))
                )
                html = driver.page_source
                logging.debug(f"Selenium fetch successful for {url}")
                return (url, html) if html else (url, None)
        except Exception as se:
            logging.warning(f"Selenium fetch failed for {url}: {str(se)}")
            return (url, None)

def normalize_anchor_text(text: str, url: str = "") -> str:
    """Normalize anchor text, falling back to URL path if empty."""
    text = text.strip().lower()
    text = re.sub(r'[^\w\s-]', '', text)  # Remove special chars except hyphens
    text = re.sub(r'\s+', ' ', text)  # Normalize spaces
    if not text and url:
        # Extract path from URL as fallback
        path = urlparse(url).path.strip('/')
        text = path.replace('/', ' ').replace('-', ' ')
    return text or "unknown"

def extract_links(html: str, base_url: str) -> Set[Tuple[str, str]]:
    """Extract links with their anchor text, ensuring hyphenated paths are captured."""
    if not html:
        return set()
    soup = BeautifulSoup(html, 'lxml')
    links_with_anchor = set()
    for a in soup.find_all('a', href=True):
        href = a.get('href', '')
        logging.debug(f"Raw href: {href}")
        full_url = urljoin(base_url, href).split('#')[0].rstrip('/')
        anchor_text = a.get_text(strip=True)
        normalized_anchor = normalize_anchor_text(anchor_text, full_url)
        logging.debug(f"Extracted link: {full_url} | Raw Anchor: {anchor_text} | Normalized Anchor: {normalized_anchor}")
        if is_valid_url(full_url):
            links_with_anchor.add((full_url, normalized_anchor))
        else:
            logging.debug(f"Filtered out invalid URL: {full_url}")
    return links_with_anchor

def is_valid_url(url: str) -> bool:
    """Validate URL format and exclude unwanted patterns."""
    parsed = urlparse(url)
    path = parsed.path.lower()
    # Exclude media, news, governance, and other irrelevant patterns
    if re.search(r"(login|signup|auth|\.pdf|\.docx?|media|news|governance|events|student|blog)", path, re.I):
        return False
    return (
        parsed.scheme in {'http', 'https'}
        and parsed.netloc
    )

def get_page_content(html: str) -> str:
    """Extract text content from HTML."""
    if not html:
        return ""
    soup = BeautifulSoup(html, 'lxml')
    return ' '.join(soup.get_text().split()[:200])  # Limit to 200 words

### Semantic Classification Function
def categorize_urls_with_semantics(links_with_anchor: Set[Tuple[str, str]], university_domains: Set[str], html: str) -> Tuple[Set[Tuple[str, str]], Set[Tuple[str, str]]]:
    """Categorize URLs using semantic similarity, with special handling for hyphenated academic paths."""
    lab_urls = set()
    startup_urls = set()

    page_content = get_page_content(html)
    page_embedding = model.encode(page_content) if page_content else None

    for url, anchor_text in links_with_anchor:
        if not is_university_domain(url, university_domains):
            logging.debug(f"Skipping non-university URL: {url}")
            continue
        # Relaxed path filter for hyphenated academic paths
        path = urlparse(url).path.lower()
        is_academic = any(kw in path for kw in ACADEMIC_KEYWORDS)
        if not is_academic:
            logging.debug(f"Skipping non-academic URL: {url}")
            continue

        context = anchor_text if anchor_text and anchor_text != "unknown" else path.replace('/', ' ').replace('-', ' ')
        context_embedding = model.encode(context)
        
        # Compute similarities
        lab_score = util.cos_sim(context_embedding, lab_embedding)[0][0]
        startup_score = util.cos_sim(context_embedding, startup_embedding)[0][0]
        
        # Boost score with page content
        if page_embedding is not None:
            page_lab_score = util.cos_sim(page_embedding, lab_embedding)[0][0]
            page_startup_score = util.cos_sim(page_embedding, startup_embedding)[0][0]
            lab_score = max(lab_score, page_lab_score * 0.5)
            startup_score = max(startup_score, page_startup_score * 0.5)
        
        # Increased boost for hyphenated academic terms
        if any(kw in (anchor_text.lower() + ' ' + path) for kw in ['faculties', 'departments']):
            lab_score += 0.3  # Higher boost for key terms
        elif any(kw in (anchor_text.lower() + ' ' + path) for kw in ACADEMIC_KEYWORDS):
            lab_score += 0.2
        
        logging.debug(f"Link: {url} | Anchor: {anchor_text} | Path: {path} | Lab Score: {lab_score:.2f} | Startup Score: {startup_score:.2f}")
        
        # Relaxed threshold for hyphenated academic paths
        threshold = 0.4 if 'faculties' in path or 'departments' in path else 0.5
        if lab_score > startup_score and lab_score > threshold:
            lab_urls.add((url, anchor_text))
        elif startup_score > lab_score and startup_score > 0.5:
            startup_urls.add((url, anchor_text))

    return lab_urls, startup_urls

### Core Processing Function
def process_directory(url: str, university_domains: Set[str], visited_urls: Set[str], depth: int = 0) -> Tuple[Set[Tuple[str, str]], Set[Tuple[str, str]]]:
    """Recursively crawl directories and categorize URLs."""
    if depth > MAX_DEPTH or url in visited_urls or len(visited_urls) >= MAX_URLS or (time.time() - start_time) > TIMEOUT_SECONDS:
        logging.info(f"Stopping at {url}: Depth {depth}, Visited {len(visited_urls)}, Time elapsed {int(time.time() - start_time)}s")
        return set(), set()
    
    if not utils.state.crawler_running_event.is_set():
        logger.info(f"Stopping at {url}: Crawler stopped by user")
        return set(), set()

    logging.info(f"Processing directory at depth {depth}: {url} (Visited: {len(visited_urls)})")
    visited_urls.add(url)
    url_lab_urls: Set[Tuple[str, str]] = set()
    url_startup_urls: Set[Tuple[str, str]] = set()

    # Fetch and parse the page
    _, html = fetch_url(url)
    if not html:
        return url_lab_urls, url_startup_urls

    # Extract links with anchor text
    links_with_anchor = extract_links(html, url)

    # Categorize using semantic analysis
    lab_urls, startup_urls = categorize_urls_with_semantics(links_with_anchor, university_domains, html)
    url_lab_urls.update(lab_urls)
    url_startup_urls.update(startup_urls)

    # Filter and prioritize sublinks
    all_sublinks = {link[0] for link in links_with_anchor}
    university_sublinks = {link for link in all_sublinks if is_university_domain(link, university_domains)}
    sublinks = university_sublinks - visited_urls
    
    # Prioritize sublinks with balanced scoring
    if sublinks:
        prioritized = []
        for link in sublinks:
            anchor = next((a for u, a in links_with_anchor if u == link), "")
            path = urlparse(link).path.lower()
            lab_score = util.cos_sim(model.encode(anchor or path.replace('/', ' ').replace('-', ' ')), lab_embedding)[0][0]
            # Increased boost for hyphenated academic URLs
            if any(kw in (anchor.lower() or path) for kw in ['faculties', 'departments']):
                lab_score += 0.3
            elif any(kw in (anchor.lower() or path) for kw in ACADEMIC_KEYWORDS):
                lab_score += 0.2
            prioritized.append((link, anchor, lab_score))
        prioritized = sorted(prioritized, key=lambda x: x[2], reverse=True)
        sublinks = [url for url, _, _ in prioritized]
        logging.info(f"Prioritized {len(sublinks)} sublinks at depth {depth} for {url}")

    if depth + 1 <= MAX_DEPTH and sublinks:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {
                executor.submit(process_directory, sub_url, university_domains, visited_urls, depth + 1): sub_url
                for sub_url in sublinks if len(visited_urls) < MAX_URLS and (time.time() - start_time) <= TIMEOUT_SECONDS
            }
            for future in as_completed(futures):
                if not utils.state.crawler_running_event.is_set():
                    logger.info("Stopping sublink processing: Crawler stopped by user")
                    break
                try:
                    sub_lab_urls, sub_startup_urls = future.result()
                    url_lab_urls.update(sub_lab_urls)
                    url_startup_urls.update(sub_startup_urls)
                except Exception as e:
                    sub_url = futures[future]
                    logging.error(f"Error processing sublink {sub_url}: {str(e)}")

    return url_lab_urls, url_startup_urls

### Main URL Generation Function
def generate_urls():
    """Generate and categorize URLs with recursive crawling and semantic analysis."""
    global start_time
    start_time = time.time()

    if not utils.state.crawler_running_event.is_set():
        logger.info("Generating URLs stopped before starting")
        return

    try:
        university_domains = load_university_domains()
        if not university_domains:
            logging.error("Aborting due to missing university domains")
            return

        try:
            with open('data/potential_directories.txt', 'r') as f:
                directory_urls = {line.strip() for line in f if line.strip()}
            directory_urls = {url for url in directory_urls if is_university_domain(url, university_domains)}
            logging.info(f"Filtered to {len(directory_urls)} university domain URLs")
            if not directory_urls:
                logging.error("No valid university domain URLs to process")
                return
        except FileNotFoundError:
            logging.error("data/potential_directories.txt not found")
            return

        all_lab_urls: Set[Tuple[str, str]] = set()
        all_startup_urls: Set[Tuple[str, str]] = set()
        visited_urls = set()

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {
                executor.submit(process_directory, url, university_domains, visited_urls): url
                for url in directory_urls
            }
            for future in as_completed(futures):
                try:
                    lab_urls, startup_urls = future.result()
                    all_lab_urls.update(lab_urls)
                    all_startup_urls.update(startup_urls)
                except Exception as e:
                    url = futures[future]
                    logging.error(f"Error processing {url}: {str(e)}")

        merged_urls = all_lab_urls | all_startup_urls
        logging.info(f"Discovered {len(merged_urls)} unique URLs")

        temp_file = 'urls.tmp'
        with open(temp_file, 'w', encoding='utf-8') as f:
            for url, anchor_text in sorted(merged_urls):
                f.write(f"{url}|{anchor_text}\n")
        os.replace(temp_file, 'data/urls.txt')
        logging.info("URL generation completed successfully")

    except Exception as e:
        logging.error(f"Critical error in URL generation: {str(e)}")
        raise
    finally:
        cleanup_selenium_driver()  # Ensure driver is closed

    if not utils.state.crawler_running_event.is_set():
        logger.info("URL generation stopped by user")
        return

### URL Loading Function
def load_seed_urls(file_path: str = 'data/urls.txt') -> List[Tuple[str, str]]:
    """Load URLs with anchor text from file."""
    urls_with_anchor = []
    seen = set()
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split('|', 1)
                url = parts[0]
                anchor_text = parts[1] if len(parts) > 1 else ""
                if url and is_valid_url(url) and url not in seen:
                    urls_with_anchor.append((url, anchor_text))
                    seen.add(url)
        logging.info(f"Loaded {len(urls_with_anchor)} validated URLs from {file_path}")
        return urls_with_anchor
    except FileNotFoundError:
        logging.error(f"URL file not found: {file_path}")
        return []



http POST http://localhost:8000/ai-prompt/ Content-Type:application/json prompt="clean energy research at KNUST"